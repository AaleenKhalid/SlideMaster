import logging
import re
from nltk.tokenize import sent_tokenize
import nltk
import requests
import time
import os
from urllib.parse import quote_plus
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


logger = logging.getLogger(__name__)

class VerificationService:
    """
    This class verifies the markdown content generated by the LLM.
    Performs some basic structure checking and some fact-checking
    """
    def __init__(self):
        pass


    def verify_markdown(self, markdown_content):
        """
        Perform basic verification on generated markdown.

        :param markdown_content: Generated markdown text
        :return: Verified markdown or raises ValueError
        """
        if not isinstance(markdown_content, str):
            markdown_content = str(markdown_content)

        if not markdown_content:
            raise ValueError("No content generated")

        if len(markdown_content) < 50:
            raise ValueError("Generated Content too short")

        if len(markdown_content) < 10:  # Minimum content length
            raise ValueError("Generated content is too short")

        # verifying the structure of the generated content
        self.verify_structure(markdown_content)

        # complete fact check on the markdown content
        self.fact_check(markdown_content)

        return markdown_content


    def verify_structure(self, markdown_content):
        """
        To verify structure of the markdown content.
        Checking stuff like, headings, the number of sections and format of the main content

        :param content: Markdown content
        :raises ValueError: If structure verification fails
        """
        # Check the headings
        if not re.search(r'^#\s+\w+', markdown_content, re.MULTILINE):
            raise ValueError("Missing the main headers in the content")

        section_count = len(re.findall(r'^#{1,3}\s+\w+', markdown_content, re.MULTILINE))
        if section_count < 5:
            raise ValueError("Content should have multiple sections")

        # Check format of the main content
        if not re.search(r'^\s*[-*]\s+\w+', markdown_content, re.MULTILINE):
            raise ValueError("Content should have bullet points")


    def fact_check(self, markdown_content):
        """
        This function will perform the fact-checking on the markdown content

        :param markdown_content: The generated content to fact-check
        :return:
        """
        logger.info("Starting fact-checking process")

        # Step 1 - Extract the key points and claims from the markdown content
        factual_statements = self.extract_key_facts(markdown_content)
        logger.info(f"Extracted {len(factual_statements)} factual statements for verification")

        if not factual_statements:
            logger.info("No factual statements found for verification")
            return {
                "verified": True,
                "message": "No factual statements found that require verification",
                "verification_summary": {
                    "total_statements": 0,
                    "verified_statements": 0,
                    "problematic_statements": []

                }
            }

        # Step 2 - Search sources for each claim using SerpAPI
        search_results = self.search_for_facts(factual_statements)
        logger.info(f"Completed search queries for {len(search_results)} statements")

        # Step 3 - Compare the responses using co-sine similarity
        verification_results = self.check_similarity(factual_statements, search_results)
        logger.info("Completed similarity Comparison for all statements")

        # Step 4 - Evaluate the overall verification results
        verification_summary = self.evaluate_verification_results(verification_results)
        logger.info(f"Verification rate: {verification_summary['verification_rate']:.2f}")

        # Want to see if the content passes fact-checking
        content_verified = verification_summary['overall_verified']

        # If it's not verified, identify the problematic statements
        if not content_verified:
            message = f"Content contains {len(verification_summary['problematic_statements'])} unverified factual statements"
        else:
            message = "Content successfully verified"

        # Return the complete fact-checking results :)
        return {
            "verified": content_verified,
            "message": message,
            "verification_summary": verification_summary,
            "verification_details": verification_results
        }



    # TODO - need to fix this function
    def extract_key_facts(self, markdown_content):
        """
        This function will extract factual claims & key points from the generated markdown content.
        :param markdown_content: Generated markdown text
        :return: the list of extracted factual claims
        """
        # I will be using the NLTK (Natural Language Toolkit) - which is a powerful python lib for working with human language data
        # 1st need to download all necessary NLTK data
        try:
            nltk.data.find('tokenizers/punkt_tab')
        except LookupError:
            nltk.download('punkt_tab')

        # Need to remove the code blocks from the markdown - headers, bullet points, etc
        content_without_code = re.sub(r'```.*?```', '', markdown_content, flags=re.DOTALL)
        content_without_headers = re.sub(r'^#{1,6}\s+.*$', '', content_without_code, flags=re.MULTILINE)
        # extracting the bullet points - most likely to contain the info we're looking for
        bullet_points = re.findall(r'^\s*[-*]\s+(.*?)$', content_without_headers, re.MULTILINE)

        # Going to get any other remaining text paragraphs
        paragraphs = re.sub(r'^\s*[-*]\s+.*$', '', content_without_headers, flags=re.MULTILINE)
        paragraphs = re.sub(r'\n\s*\n+', '\n\n', paragraphs).strip()

        # Need to tokenise the paragraphs into sentences
        sentences = []
        for paragraph in paragraphs.split('\n\n'):
            if paragraph.strip():
                sentences.extend(sent_tokenize(paragraph.strip()))

        # Combine the bullet points and the sentences -> will make up all possible factual claims made
        potential_facts = bullet_points + sentences

        # Now need to filter the claims that might actually be factual
        # Basically looking for statements wwith numbers, dates, stats or just factual indicators
        fact_patterns = [
            r'\d+%',                      # Percentages
            r'in \d{4}',                  # Years
            r'according to',              # Citations
            r'research shows',            # Research references
            r'studies indicate',          # Study references
            r'statistics reveal',         # Statistical references
            r'data shows',                # Data references
            r'evidence suggests',         # Evidence references
            r'experts agree',             # Expert references
            r'\d+ (million|billion)',     # Large numbers
            r'increased by',              # Trend statements
            r'decreased by',              # Trend statements
            r'discovered',                # Discoveries
            r'invented',                  # Inventions
            r'founded',                   # Founding events
            r'established'                # Establishment events
        ]

        factual_statements = []
        for statement in potential_facts:
            statement = statement.strip()
            if not statement:
                continue

            # might have too many factual claims, so want to prioritise the stronger ones
            if len(factual_statements) > 10:
                # Going to sort by their likelyhood of being a strong claim
                def fact_strength(statement):
                    score = 0
                    for pattern in fact_patterns:
                        if re.search(pattern, statement, re.IGNORECASE):
                            score += 1
                    # Want to prioritise shorter and concise statements too
                    return score - (len(statement) / 1000) # adding a penalty for the length

                factual_statements.sort(key=fact_strength, reverse=True)
                factual_statements = factual_statements[:10] # going to limit it to 10 claims

            return factual_statements


    def search_for_facts(self, factual_statements):
        """
        Going to use SerpAPI to search for verification of the claims

        :param factual_statements: List of the claims made in the generated markdown content
        :return: Dictionary mapping statements to the search results
        """

        # Get the API key
        api_key = os.getenv('SERPAPI_KEY')
        if not api_key:
            logger.error("SERPAPI_KEY environment variable is not set")
            raise ValueError("SerpAPI key not configured. No API key provided")

        search_results = {}

        for statement in factual_statements:
            # Going to formulate a search query
            search_query = f"fact check {statement}"

            # Create the API url
            url = f"https://serpapi.com/search.json?q={quote_plus(search_query)}&api_key={api_key}"

            try:
                # Try making the API request
                logger.info(f"Searching for: {search_results}...")
                response = requests.get(url)
                response.raise_for_status() # raise exception for HTTP errors

                # Parse the response
                search_data = response.json()

                # Need to store the results
                search_results[statement] = self.extract_relevant_results(search_data, statement)

                # Going to add a delay in regards to the rate limits
                time.sleep(1)
            except requests.exceptions.RequestException as e:
                logger.error(f"Error searching for '{statement}...' : {str(e)}")
                search_results[statement] = {""}

        return search_results


    def extract_relevant_results(self, search_data, statement):
        """
        Function to extract the relevant information from the SerpAPI reponse

        :param search_data: The full API response data
        :param statement: The original claim that's being checked
        :return: Dictionary of relevant search results
        """

        relevant_results = {
            "organic results": [],
            "knowledge_graph": None,
            "answer_box": None,
            "related_questions": []
        }

        # Extract the organic search results (usually 10 but I'm going to work with 5)
        if "organic results" in search_data:
            for result in search_data["organic results"][:5]: #top 5 for relevance
                relevant_results["organic results"].append({
                    "title": result.get("title", ""),
                    "link": result.get("link", ""),
                    "snippet": result.get("snippet", ""),
                    "source": result.get("source", "")
                })

        # Going to extract knowledge graph info if available
        if "knowledge_graph" in search_data:
            kg = search_data["knowledge_graph"]
            relevant_results["knowledge_graph"] = {
                "title": kg.get("title", ""),
                "description": kg.get("description", ""),
                "source": kg.get("source", {}).get("link", "")
            }

        # Going to extract featured snippet or answer box if available
        if "answer_box" in search_data:
            ab = search_data["answer_box"]
            relevant_results["answer_box"] = {
                "title": ab.get("title", ""),
                "answer": ab.get("answer", ab.get("snippet", "")),
                "source": ab.get("link", "")
            }

        # Going to extract related questions (if available)
        if "related_questions" in search_data:
            for question in search_data["related_questions"][:3]: # going to limit to top 3
                relevant_results["related_questions"].append({
                    "question": question.get("question", ""),
                    "answer": question.get("answer", ""),
                    "source": question.get("source", {}).get("link", "")
                })


        return relevant_results


    def check_similarity(self, factual_statements, search_results):
        """
        Compare the factual claims with the search results using cosine similarity.

        :param factual_statements: List of the extracted factual claims
        :param search_results: Dictionary of the search results for each statement
        :return: Dictionary with verification results for each statement
        """

        verification_results = {}

        for statement in factual_statements:
            if statement not in search_results:
                verification_results[statement] = {
                    "verified": False,
                    "confidence": 0.0,
                    "sources": [],
                    "error": "No search results available"
                }
                continue

            # Collect all relevant text from the search results
            reference_texts = []
            sources = []

            # need to add organic search results
            for result in search_results[statement]["organic_results"]:
                if result["snippet"]:
                    reference_texts.append(result["snippet"])
                    sources.append({
                        "type": "web_result",
                        "title": result["title"],
                        "url": result["link"],
                        "snippet": result["snippet"]
                    })

            if search_results[statement]["knowledge_graph"]:
                kg = search_results[statement]["knowledge_graph"]
                if kg["description"]:
                    reference_texts.append(kg["description"])
                    sources.append({
                        "type": "knowledge_graph",
                        "title": kg["title"],
                        "url": kg["source"],
                        "snippet": kg["description"]
                    })

            # add the answer box if available
            if search_results[statement]["answer_box"]:
                ab = search_results[statement]["answer_box"]
                if ab["answer"]:
                    reference_texts.append(ab["answer"])
                    sources.append({
                        "type": "answer_box",
                        "title": ab["title"],
                        "url": ab["source"],
                        "snippet": ab["answer"]
                    })

            # add any related questions
            for question in search_results[statement]["related_questions"]:
                if question["answer"]:
                    reference_texts.append(question["answer"])
                    sources.append({
                        "type": "related_question",
                        "title": question["question"],
                        "url": question["source"],
                        "snippet": question["answer"]
                    })

            if not reference_texts:
                verification_results[statement] = {
                    "verified": False,
                    "confidence": 0.0,
                    "sources": [],
                    "error": "No relevant text found in search results"
                }
                continue

            # Now going to try and calculate the co-sine similarity
            try:
                # create tf-idf vectors
                vectorizer = TfidfVectorizer().fit_transform([statement] + reference_texts)
                vectors = vectorizer.toarray()

                # calc the co-sine similarity
                statement_vector = vectors[0:1]
                reference_vectors = vectors[1:]

                similarities = cosine_similarity(statement_vector, reference_vectors)#[0] #TODO - fix this

                # Get the max similarity score
                max_similarity = np.max(similarities) if len(similarities) > 0 else 0.0

                # Want to get the index of the most similar reference
                if len(similarities) > 0:
                    most_similar_idx = np.argmax(similarities)
                    best_source = sources[most_similar_idx] if most_similar_idx < len(sources) else None
                else:
                    best_source = None


                # Going to define verification thresholds
                verified = max_similarity >= 0.3 #0.6 # TODO - might need to adjust this
                high_confidence = max_similarity >= 0.75

                # prep the verification result
                verification_results[statement] = {
                    "verified": verified,
                    "confidence": float(max_similarity),
                    "high_confidence": high_confidence,
                    "best_source": best_source,
                    "sources": sources
                }
            except Exception as e:
                logger.error(f"Error calculating similarity for '{statement[:100]}...': {str(e)}")
                verification_results[statement] = {
                    "verified": False,
                    "confidence": 0.0,
                    "sources": sources,
                    "error": str(e)
                }

        return verification_results


    def evaluate_verification_results(self, verification_results):
        """
        This function evaluates the overall verification results and identifies the problematic statements

        :param verification_results: Dictionary with the verification results for the statements
        :return: Dictionary with overall verification summary
        """
        total_statements = len(verification_results)
        verified_statements = sum(1 for result in verification_results.values() if result.get("verified", False))
        high_confidence_statements = sum(1 for result in verification_results.values()
                                         if result.get("high_confidence", False))

        problematic_statements = [
            {
                "statement": statement,
                "confidence": result.get("confidence", 0.0),
                "best_source": result.get("best_source")
            }
            for statement, result in verification_results.items()
            if not result.get("verified", False) and "error" not in result
        ]

        error_statements = [
            {
                "statement": statement,
                "error": result.get("error")
            }
            for statement, result in verification_results.items()
            if "error" in result
        ]

        return {
            "total_statements": total_statements,
            "verified_statements": verified_statements,
            "verification_rate": verified_statements / total_statements if total_statements > 0 else 0,
            "high_confidence_statements": high_confidence_statements,
            "problematic_statements": problematic_statements,
            "error_statements": error_statements,
            "overall_verified": verified_statements / total_statements >= 0.7 if total_statements > 0 else False
        }
